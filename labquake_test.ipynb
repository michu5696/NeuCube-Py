{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ddeecf-beb3-4e07-ac81-53dd325ba13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neucube import Reservoir\n",
    "from neucube.encoder import RateEncoder\n",
    "from neucube.validation import Pipeline\n",
    "from neucube.sampler import SpikeCount, DeSNN\n",
    "from neucube.datamanager import DataManager\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662ae5c4-88b5-4edd-bdb1-4cbe6db6c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'source_data_path': 'example_data/labquake_source',\n",
    "            'samples_path':'example_data/labquake_samples',\n",
    "            'sampling_rate': 5000,\n",
    "            'batch_duration': 12000,\n",
    "        }\n",
    "datamanager = DataManager(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecd3687-3bee-4c0c-a7f9-d392c6c4aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datamanager.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f325c6-b1c5-41ca-8892-98ff7f1369ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of CSV files with growing 'n'\n",
    "#num_files = 270  # specify the number of files\n",
    "#column_name = 'Channel_13'  # specify the column to plot\n",
    "#\n",
    "## Initialize plot\n",
    "#plt.figure(figsize=(15,5))\n",
    "#\n",
    "## Initialize the starting index for plotting\n",
    "#start_index = 0\n",
    "#\n",
    "## Loop over the files\n",
    "#for n in range(1, num_files + 1):\n",
    "#    # Construct file name\n",
    "#    file_name = f'example_data/labquake_samples/sample_{n}.csv'\n",
    "#    \n",
    "#    # Read CSV file\n",
    "#    df = pd.read_csv(file_name)\n",
    "#    \n",
    "#    # Get the length of the current column\n",
    "#    column_length = len(df[column_name])\n",
    "#    \n",
    "#    # Create a new index range that continues from where the last one left off\n",
    "#    index_range = range(start_index, start_index + column_length)\n",
    "#    \n",
    "#    # Plot the column from each CSV file using the new index range\n",
    "#    plt.plot(index_range, df[column_name])\n",
    "#    \n",
    "#    # Update the start_index for the next file\n",
    "#    start_index += column_length\n",
    "#\n",
    "## Add labels and title (no legend)\n",
    "#plt.xlabel('Index')\n",
    "#plt.ylabel('Value')\n",
    "#plt.title(f'Plot of {column_name} across multiple files')\n",
    "#\n",
    "## Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f81b2c-4c64-4db5-a8a0-ff4d6e75a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_csv_row_sizes(directory):\n",
    "    # List all CSV files in the specified directory\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    \n",
    "    # Dictionary to store file names and their respective row counts\n",
    "    row_sizes = {}\n",
    "    \n",
    "    # Iterate over each file, read it with pandas, and get the number of rows\n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            row_sizes[file_name] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "    \n",
    "    # Print row counts for each file\n",
    "    for file_name, count in row_sizes.items():\n",
    "        print(f\"{file_name}: {count} rows\")\n",
    "\n",
    "# Example usage:\n",
    "#directory_path = params['samples_path']\n",
    "#check_csv_row_sizes(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8ec8aa-8d2f-4650-9e42-cea802344242",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameslist = ['sample_'+str(idx)+'.csv' for idx in range(1,273)]\n",
    "\n",
    "dfs = []\n",
    "for filename in filenameslist:\n",
    "  dfs.append(pd.read_csv('./example_data/labquake_samples/'+filename, header=0))\n",
    "\n",
    "fulldf = pd.concat(dfs)\n",
    "\n",
    "# Load the CSV file\n",
    "labels = pd.read_csv('./example_data/labquake_samples/all_class_labels.csv')\n",
    "\n",
    "# Extract each column into a separate 1D array\n",
    "y1 = labels['Zone1'].values\n",
    "y2 = labels['Zone2'].values\n",
    "y3 = labels['Zone3'].values\n",
    "y4 = labels['Zone4'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79515db9-d68c-46a1-aa0e-6caee969b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zone 1: 5it [01:58, 23.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Zone 1:\n",
      "Accuracy: 0.4889705882352941\n",
      "Confusion Matrix:\n",
      "[[74  3 17 17]\n",
      " [ 4  3  5  8]\n",
      " [20  7 11 18]\n",
      " [19  4 17 45]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zone 2: 5it [01:59, 23.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Zone 2:\n",
      "Accuracy: 0.4632352941176471\n",
      "Confusion Matrix:\n",
      "[[77 18 21  2]\n",
      " [18 24 16  1]\n",
      " [29 14 22  4]\n",
      " [ 7  8  8  3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zone 3: 5it [01:59, 23.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Zone 3:\n",
      "Accuracy: 0.5845588235294118\n",
      "Confusion Matrix:\n",
      "[[111  31   7   2]\n",
      " [ 40  43   3   0]\n",
      " [  5  12   5   3]\n",
      " [  7   1   2   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zone 4: 5it [01:58, 23.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Zone 4:\n",
      "Accuracy: 0.43014705882352944\n",
      "Confusion Matrix:\n",
      "[[76 21 13 10]\n",
      " [25 13 10  3]\n",
      " [24 14 20  8]\n",
      " [12  2 13  8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading data\n",
    "X = torch.tensor(fulldf.values.reshape(272, 600, 25))  # Check if reshaping is correctly matching your dataset\n",
    "derivatives = torch.diff(X, dim=1)\n",
    "min_values = derivatives.min(dim=1)[0].min(dim=0)[0]\n",
    "max_values = derivatives.max(dim=1)[0].max(dim=0)[0]\n",
    "\n",
    "encoder = RateEncoder(min_values, max_values, max_rate=1.0)\n",
    "X = encoder.encode(X)  # Ensure this returns a tensor or array suitable for model input\n",
    "\n",
    "labels = pd.read_csv('./example_data/labquake_samples/all_class_labels.csv')\n",
    "ys = [labels[col].values for col in labels]  # Extract each column into a list of arrays\n",
    "\n",
    "# Set up K-Folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "results = {}\n",
    "\n",
    "for y_idx, y in enumerate(ys, start=1):\n",
    "    y_total, pred_total = [], []\n",
    "\n",
    "    for train_index, test_index in tqdm(kf.split(X), desc=f'Zone {y_idx}'):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        res = Reservoir(inputs=25)\n",
    "        #sam = SpikeCount()\n",
    "        sam = DeSNN() \n",
    "        clf = LogisticRegression(solver='liblinear')\n",
    "        pipe = Pipeline(res, sam, clf)\n",
    "        \n",
    "        pipe.fit(X_train, y_train)\n",
    "        pred = pipe.predict(X_test)\n",
    "\n",
    "        y_total.extend(y_test)\n",
    "        pred_total.extend(pred)\n",
    "    \n",
    "    acc = accuracy_score(y_total, pred_total)\n",
    "    cm = confusion_matrix(y_total, pred_total)\n",
    "    results[f'Zone {y_idx}'] = {'accuracy': acc, 'confusion_matrix': cm}\n",
    "    print(f\"Results for Zone {y_idx}:\")\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea4223-1a30-47ef-a82c-6f079c8f1c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
